{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch scikit-learn numpy wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "mlflow.set_experiment(\"Activation Steering Experiment with Gemma\")\n",
    "run = mlflow.start_run()\n",
    "mlflow.log_param(\"model\", \"Gemma-2-2B\")\n",
    "mlflow.log_param(\"dataset\", \"SST-2 (GLUE)\")\n",
    "mlflow.log_param(\"concept\", \"positive sentiment\")\n",
    "\n",
    "wandb.init(project=\"activation_steering_experiment\", config={\n",
    "    \"model\": \"Gemma-2-2B\",\n",
    "    \"dataset\": \"SST-2 (GLUE)\",\n",
    "    \"experiment\": \"Activation Steering with Public SAEs and 50-token rollout\"\n",
    "})\n",
    "\n",
    "# gemma model from Hugging Face \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"google/gemma-2b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
    "model.eval()\n",
    "\n",
    "from transformers import AutoModel\n",
    "# Gemma Scope SAEs\n",
    "sae_model_name = \"google/gemma-scope-2b-pt-res\"\n",
    "sae_model = AutoModel.from_pretrained(sae_model_name)\n",
    "\n",
    "'''The gemma-scope-2b-pt-res repo contains SAE weights trained on the res stream of the Gemma 2B model.\n",
    "Need to ensure that the SAE model variant aligns with the model type.\n",
    "https://huggingface.co/google/gemma-scope'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "n_samples = 200\n",
    "train_sentences = dataset['train']['sentence'][:n_samples]\n",
    "train_labels = dataset['train']['label'][:n_samples]\n",
    "print(f\"Using {len(train_sentences)} samples for the experiment.\")\n",
    "mlflow.log_metric(\"num_samples\", len(train_sentences))\n",
    "\n",
    "def extract_hidden_state(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    # avg final layer hidden states over the sequence length\n",
    "    hidden = outputs.hidden_states[-1].mean(dim=1).squeeze().detach().numpy()\n",
    "    return hidden\n",
    "\n",
    "hidden_states = []\n",
    "for text in train_sentences:\n",
    "    try:\n",
    "        h = extract_hidden_state(text)\n",
    "        hidden_states.append(h)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting hidden state for text: {text} - {e}\")\n",
    "\n",
    "hidden_states = np.array(hidden_states)\n",
    "labels = np.array(train_labels)\n",
    "print(\"Extracted hidden states shape:\", hidden_states.shape)\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(hidden_states, labels)\n",
    "C = clf.coef_.flatten()\n",
    "print(\"Trained linear probe. Learned concept (steering) vector C shape:\", C.shape)\n",
    "mlflow.log_metric(\"C_norm\", np.linalg.norm(C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tokens = [\"positive\", \"good\", \"great\", \"amazing\", \"excellent\"]\n",
    "W_pos_vectors = []\n",
    "for token in positive_tokens:\n",
    "    token_id = tokenizer.encode(token)[0]\n",
    "    vec = model.transformer.wte.weight[token_id].detach().numpy()\n",
    "    W_pos_vectors.append(vec)\n",
    "W_pos_vectors = np.stack(W_pos_vectors)\n",
    "print(\"Collected positive token unembedding vectors shape:\", W_pos_vectors.shape)\n",
    "mlflow.log_param(\"positive_tokens\", positive_tokens)\n",
    "\n",
    "#mean vector and first principal component for token embeddings\n",
    "W_pos_mean = np.mean(W_pos_vectors, axis=0)\n",
    "pca = PCA(n_components=1)\n",
    "pca.fit(W_pos_vectors)\n",
    "W_pos_pc1 = pca.components_[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "sim_mean = cosine_similarity(C, W_pos_mean)\n",
    "sim_pc1 = cosine_similarity(C, W_pos_pc1)\n",
    "print(\"Cosine similarity between C and W_pos_mean:\", sim_mean)\n",
    "print(\"Cosine similarity between C and W_pos_pc1:\", sim_pc1)\n",
    "mlflow.log_metric(\"cosine_similarity_mean\", sim_mean)\n",
    "mlflow.log_metric(\"cosine_similarity_pc1\", sim_pc1)\n",
    "\n",
    "# compare public SAE with the learned concept vector\n",
    "if gemma_positive_sae is not None:\n",
    "    sim_public = cosine_similarity(C, gemma_positive_sae)\n",
    "    print(\"Cosine similarity between learned C and Gemma public SAE (positive):\", sim_public)\n",
    "    mlflow.log_metric(\"cosine_similarity_public_SAE\", sim_public)\n",
    "else:\n",
    "    print(\"No public SAE available for comparison.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intervene_and_generate(text, steering_vector, alpha=1.0):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, output_hidden_states=True)\n",
    "    hidden = outputs.hidden_states[-1].mean(dim=1).squeeze().detach()\n",
    "    # add the scaled steering vector\n",
    "    hidden_modified = hidden + alpha * torch.tensor(steering_vector, dtype=hidden.dtype)\n",
    "    logits = hidden_modified @ model.transformer.wte.weight.T\n",
    "    next_token_id = torch.argmax(logits).item()\n",
    "    next_token = tokenizer.decode([next_token_id])\n",
    "    return next_token\n",
    "\n",
    "sample_text = \"The movie was\"\n",
    "gen_C = intervene_and_generate(sample_text, C, alpha=1.0)\n",
    "gen_Wpos = intervene_and_generate(sample_text, W_pos_mean, alpha=1.0)\n",
    "print(\"Generated token with steering C:\", gen_C)\n",
    "print(\"Generated token with steering W_pos_mean:\", gen_Wpos)\n",
    "mlflow.log_param(\"generated_token_C\", gen_C)\n",
    "mlflow.log_param(\"generated_token_Wpos\", gen_Wpos)\n",
    "\n",
    "# caa function\n",
    "def compute_caa(text, concept_vector, alpha=1.0):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    baseline_out = model(**inputs, output_hidden_states=True)\n",
    "    baseline_hidden = baseline_out.hidden_states[-1].mean(dim=1).squeeze().detach()\n",
    "    baseline_logits = baseline_hidden @ model.transformer.wte.weight.T\n",
    "    baseline_probs = torch.softmax(baseline_logits, dim=-1)\n",
    "    \n",
    "    intervened_hidden = baseline_hidden + alpha * torch.tensor(concept_vector, dtype=baseline_hidden.dtype)\n",
    "    intervened_logits = intervened_hidden @ model.transformer.wte.weight.T\n",
    "    intervened_probs = torch.softmax(intervened_logits, dim=-1)\n",
    "    \n",
    "    differences = {}\n",
    "    for token in positive_tokens:\n",
    "        token_id = tokenizer.encode(token)[0]\n",
    "        diff = intervened_probs[token_id].item() - baseline_probs[token_id].item()\n",
    "        differences[token] = diff\n",
    "    return differences\n",
    "\n",
    "caa_diffs = compute_caa(sample_text, C, alpha=1.0)\n",
    "print(\"CAA differences for sample text:\", caa_diffs)\n",
    "mlflow.log_metric(\"caa_diff_positive\", np.mean(list(caa_diffs.values())))\n",
    "\n",
    "# plot CAA diffs\n",
    "# \n",
    "fig, ax = plt.subplots()\n",
    "tokens = list(caa_diffs.keys())\n",
    "diffs = [caa_diffs[t] for t in tokens]\n",
    "ax.bar(tokens, diffs)\n",
    "ax.set_ylabel(\"Probability Difference\")\n",
    "ax.set_title(\"CAA: Change in Token Probabilities After Intervention\")\n",
    "plt.tight_layout()\n",
    "wandb.log({\"CAA_Probability_Differences\": wandb.Image(fig)})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout(prompt, steering_vector=None, alpha=1.0, length=50):\n",
    "    generated_tokens = []\n",
    "    current_prompt = prompt\n",
    "    for i in range(length):\n",
    "        inputs = tokenizer(current_prompt, return_tensors=\"pt\")\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        hidden = outputs.hidden_states[-1].mean(dim=1).squeeze().detach()\n",
    "        if steering_vector is not None:\n",
    "            hidden = hidden + alpha * torch.tensor(steering_vector, dtype=hidden.dtype)\n",
    "        logits = hidden @ model.transformer.wte.weight.T\n",
    "        next_token_id = torch.argmax(logits).item()\n",
    "        next_token = tokenizer.decode([next_token_id])\n",
    "        generated_tokens.append(next_token)\n",
    "        current_prompt += next_token\n",
    "    return generated_tokens\n",
    "\n",
    "baseline_rollout = generate_rollout(\"The movie was\", steering_vector=None, length=50)\n",
    "intervened_rollout = generate_rollout(\"The movie was\", steering_vector=C, alpha=1.0, length=50)\n",
    "\n",
    "print(\"Baseline Rollout:\\n\", \"\".join(baseline_rollout))\n",
    "print(\"\\nIntervened Rollout (with C):\\n\", \"\".join(intervened_rollout))\n",
    "\n",
    "wandb.log({\n",
    "    \"Baseline_Rollout\": \"\".join(baseline_rollout),\n",
    "    \"Intervened_Rollout\": \"\".join(intervened_rollout)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()\n",
    "wandb.finish()\n",
    "print(\"Experiment run logged with MLflow and WandB.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
