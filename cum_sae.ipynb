{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# install req pkgs: torch, datasets, transformers, scikit-learn, numpy, wandb, sae_lens\n",
        "!pip install torch datasets transformers scikit-learn numpy wandb sae_lens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUCXA5mGpnGY",
        "outputId": "3f144454-e899-447b-d8e5-d047cd40b129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.8)\n",
            "Collecting sae_lens\n",
            "  Downloading sae_lens-5.6.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.6)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.24.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Collecting automated-interpretability<1.0.0,>=0.0.5 (from sae_lens)\n",
            "  Downloading automated_interpretability-0.0.8-py3-none-any.whl.metadata (822 bytes)\n",
            "Collecting babe<0.0.8,>=0.0.7 (from sae_lens)\n",
            "  Downloading babe-0.0.7-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: matplotlib<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (3.10.0)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (0.1.7)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (3.9.1)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.19.0 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (5.24.1)\n",
            "Collecting plotly-express<0.5.0,>=0.4.1 (from sae_lens)\n",
            "  Downloading plotly_express-0.4.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting pytest-profiling<2.0.0,>=1.7.0 (from sae_lens)\n",
            "  Downloading pytest_profiling-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting python-dotenv<2.0.0,>=1.0.1 (from sae_lens)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pyzmq==26.0.0 (from sae_lens)\n",
            "  Downloading pyzmq-26.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.2 kB)\n",
            "Collecting safetensors>=0.4.3 (from transformers)\n",
            "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: simple-parsing<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from sae_lens) (0.1.7)\n",
            "Collecting transformer-lens<3.0.0,>=2.0.0 (from sae_lens)\n",
            "  Downloading transformer_lens-2.15.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting typer<0.13.0,>=0.12.3 (from sae_lens)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting zstandard<0.23.0,>=0.22.0 (from sae_lens)\n",
            "  Downloading zstandard-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec (from torch)\n",
            "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting blobfile<3.0.0,>=2.1.1 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading blobfile-2.1.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting boostedblob<0.16.0,>=0.15.3 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading boostedblob-0.15.6-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from automated-interpretability<1.0.0,>=0.0.5->sae_lens) (3.10.15)\n",
            "Collecting tiktoken>=0.6.0 (from automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting py2store (from babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading py2store-0.1.20.tar.gz (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.1/143.1 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting graze (from babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading graze-0.1.29-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4.0.0,>=3.8.3->sae_lens) (2.8.2)\n",
            "Requirement already satisfied: traitlets in /usr/local/lib/python3.11/dist-packages (from matplotlib-inline<0.2.0,>=0.1.6->sae_lens) (5.7.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.19.0->sae_lens) (9.0.0)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae_lens) (0.14.4)\n",
            "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.11/dist-packages (from plotly-express<0.5.0,>=0.4.1->sae_lens) (1.0.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from pytest-profiling<2.0.0,>=1.7.0->sae_lens) (8.3.5)\n",
            "Collecting gprof2dot (from pytest-profiling<2.0.0,>=1.7.0->sae_lens)\n",
            "  Downloading gprof2dot-2024.6.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple-parsing<0.2.0,>=0.1.6->sae_lens) (0.16)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformer-lens<3.0.0,>=2.0.0->sae_lens) (1.5.2)\n",
            "Collecting beartype<0.15.0,>=0.14.1 (from transformer-lens<3.0.0,>=2.0.0->sae_lens)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting better-abc<0.0.4,>=0.0.3 (from transformer-lens<3.0.0,>=2.0.0->sae_lens)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer-lens<3.0.0,>=2.0.0->sae_lens) (0.8.1)\n",
            "Collecting fancy-einsum>=0.0.3 (from transformer-lens<3.0.0,>=2.0.0->sae_lens)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer-lens<3.0.0,>=2.0.0->sae_lens)\n",
            "  Downloading jaxtyping-0.3.0-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.11/dist-packages (from transformer-lens<3.0.0,>=2.0.0->sae_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from transformer-lens<3.0.0,>=2.0.0->sae_lens) (0.2.0)\n",
            "Collecting transformers-stream-generator<0.0.6,>=0.0.5 (from transformer-lens<3.0.0,>=2.0.0->sae_lens)\n",
            "  Downloading transformers-stream-generator-0.0.5.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.11/dist-packages (from transformer-lens<3.0.0,>=2.0.0->sae_lens) (4.4.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.3->sae_lens) (1.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting pycryptodomex~=3.8 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading pycryptodomex-3.22.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting lxml~=4.9 (from blobfile<3.0.0,>=2.1.1->automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading lxml-4.9.4-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
            "Collecting uvloop>=0.16.0 (from boostedblob<0.16.0,>=0.15.3->automated-interpretability<1.0.0,>=0.0.5->sae_lens)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->automated-interpretability<1.0.0,>=0.0.5->sae_lens) (0.14.0)\n",
            "Collecting wadler-lindig>=0.1.3 (from jaxtyping>=0.2.11->transformer-lens<3.0.0,>=2.0.0->sae_lens)\n",
            "  Downloading wadler_lindig-0.1.4-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer-lens<3.0.0,>=2.0.0->sae_lens) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.6.0->transformer-lens<3.0.0,>=2.0.0->sae_lens) (2.18.0)\n",
            "Collecting dol (from graze->babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading dol-0.3.16-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting config2py (from py2store->babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading config2py-0.1.37-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from py2store->babe<0.0.8,>=0.0.7->sae_lens) (6.5.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae_lens) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->pytest-profiling<2.0.0,>=1.7.0->sae_lens) (1.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens<3.0.0,>=2.0.0->sae_lens) (0.1.2)\n",
            "Collecting i2 (from config2py->py2store->babe<0.0.8,>=0.0.7->sae_lens)\n",
            "  Downloading i2-0.1.46-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m446.8/664.8 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:05\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from sae_lens import SAE\n",
        "\n",
        "# set seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# init wandb (mlflow commented out)\n",
        "wandb.init(project=\"activation_steering_experiment\", config={\n",
        "    \"model\": \"google/gemma-2b\",\n",
        "    \"dataset\": \"sst2 (glue)\",\n",
        "    \"exp\": \"act steer w/ gemma saes & 50-token rollout\"\n",
        "})\n"
      ],
      "metadata": {
        "id": "KxjU32EDplZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# login to hf (uncomment and enter token if needed)\n",
        "#!huggingface-cli login"
      ],
      "metadata": {
        "id": "cCBESUyqpsiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load gemma model & tokenizer\n",
        "model_name = \"google/gemma-2b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, output_hidden_states=True)\n",
        "model.eval()\n",
        "\n",
        "# load sae from gemma-scope release (actual id, no placeholder)\n",
        "# this loads the sae for layer0 with width 16k from the canonical release\n",
        "sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
        "    release=\"gemma-scope-2b-pt-res-canonical\",\n",
        "    sae_id=\"layer_0/width_16k/canonical\",\n",
        ")\n",
        "print(\"loaded sae: cfg:\", cfg_dict, \"sparsity:\", sparsity)\n",
        "\n",
        "# load sst2 dataset (glue)\n",
        "dataset = load_dataset(\"glue\", \"sst2\")\n",
        "n_samples = 200\n",
        "train_sentences = dataset['train']['sentence'][:n_samples]\n",
        "train_labels = dataset['train']['label'][:n_samples]\n",
        "print(f\"using {len(train_sentences)} samples for exp\")\n"
      ],
      "metadata": {
        "id": "8yRP4uELpwdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use layer0 for sae encoding\n",
        "layer_index = 0\n",
        "\n",
        "# extract hidden state with sae encoding: tokenize, get layer0, pass thru sae, avg tokens\n",
        "def extract_hidden_state_with_sae(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "    # get hidden states from layer_index\n",
        "    hidden = outputs.hidden_states[layer_index]\n",
        "    # pass through sae: returns sparse representation with same shape\n",
        "    sae_out = sae.encode(hidden)\n",
        "    # avg over token seq (dim=1)\n",
        "    rep = sae_out.mean(dim=1).squeeze().detach().numpy()\n",
        "    return rep\n",
        "\n",
        "# get sae-encoded hidden states for all train samples\n",
        "hidden_states = []\n",
        "for txt in train_sentences:\n",
        "    try:\n",
        "        h = extract_hidden_state_with_sae(txt)\n",
        "        hidden_states.append(h)\n",
        "    except Exception as e:\n",
        "        print(\"err on txt:\", txt, e)\n",
        "hidden_states = np.array(hidden_states)\n",
        "labels = np.array(train_labels)\n",
        "print(\"extracted sae-hidden shape:\", hidden_states.shape)"
      ],
      "metadata": {
        "id": "_lM_cLMdpyyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train linear probe on sae-hidden states (logistic regression)\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(hidden_states, labels)\n",
        "C = clf.coef_.flatten()  # learned concept vector\n",
        "print(\"trained probe, c shape:\", C.shape)"
      ],
      "metadata": {
        "id": "t_2dD2Kyp2Gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract positive sentiment token unembedding vectors\n",
        "positive_tokens = [\"positive\", \"good\", \"great\", \"amazing\", \"excellent\"]\n",
        "W_pos_vectors = []\n",
        "for tok in positive_tokens:\n",
        "    tok_id = tokenizer.encode(tok)[0]\n",
        "    vec = model.transformer.wte.weight[tok_id].detach().numpy()\n",
        "    W_pos_vectors.append(vec)\n",
        "W_pos_vectors = np.stack(W_pos_vectors)\n",
        "print(\"pos token unemb shape:\", W_pos_vectors.shape)\n",
        "\n",
        "# aggregate: mean & pca (pc1)\n",
        "W_pos_mean = np.mean(W_pos_vectors, axis=0)\n",
        "pca = PCA(n_components=1)\n",
        "pca.fit(W_pos_vectors)\n",
        "W_pos_pc1 = pca.components_[0]"
      ],
      "metadata": {
        "id": "_-wa8tfqp5mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine sim func\n",
        "def cosine_similarity(a, b):\n",
        "    return np.dot(a, b) / (np.linalg.norm(a)*np.linalg.norm(b))\n",
        "\n",
        "sim_mean = cosine_similarity(C, W_pos_mean)\n",
        "sim_pc1 = cosine_similarity(C, W_pos_pc1)\n",
        "print(\"cosine sim: c & w_pos_mean:\", sim_mean)\n",
        "print(\"cosine sim: c & w_pos_pc1:\", sim_pc1)"
      ],
      "metadata": {
        "id": "vp80Gza7p8JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# act steer intervention: add steering vector to avg sae-hidden state of text, then decode next token\n",
        "def intervene_and_generate(text, steering_vector, alpha=1.0):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "    # use final layer hidden state avg (using sae encoding optional, here use raw for generation)\n",
        "    hidden = outputs.hidden_states[-1].mean(dim=1).squeeze().detach()\n",
        "    hidden_mod = hidden + alpha * torch.tensor(steering_vector, dtype=hidden.dtype)\n",
        "    logits = hidden_mod @ model.transformer.wte.weight.T\n",
        "    next_id = torch.argmax(logits).item()\n",
        "    next_tok = tokenizer.decode([next_id])\n",
        "    return next_tok"
      ],
      "metadata": {
        "id": "GSJWktSPqAct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"the movie was\"\n",
        "gen_C = intervene_and_generate(sample_text, C, alpha=1.0)\n",
        "gen_Wpos = intervene_and_generate(sample_text, W_pos_mean, alpha=1.0)\n",
        "print(\"gen token w/ c:\", gen_C)\n",
        "print(\"gen token w/ w_pos_mean:\", gen_Wpos)"
      ],
      "metadata": {
        "id": "s2lwkryPqE1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute controlled activation attribution (caa) diff for positive tokens\n",
        "def compute_caa(text, concept_vector, alpha=1.0):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    base_out = model(**inputs, output_hidden_states=True)\n",
        "    base_hidden = base_out.hidden_states[-1].mean(dim=1).squeeze().detach()\n",
        "    base_logits = base_hidden @ model.transformer.wte.weight.T\n",
        "    base_probs = torch.softmax(base_logits, dim=-1)\n",
        "\n",
        "    mod_hidden = base_hidden + alpha * torch.tensor(concept_vector, dtype=base_hidden.dtype)\n",
        "    mod_logits = mod_hidden @ model.transformer.wte.weight.T\n",
        "    mod_probs = torch.softmax(mod_logits, dim=-1)\n",
        "\n",
        "    diffs = {}\n",
        "    for tok in positive_tokens:\n",
        "        tok_id = tokenizer.encode(tok)[0]\n",
        "        diffs[tok] = mod_probs[tok_id].item() - base_probs[tok_id].item()\n",
        "    return diffs\n",
        "\n",
        "caa_diffs = compute_caa(sample_text, C, alpha=1.0)\n",
        "print(\"caa diffs:\", caa_diffs)\n",
        "\n",
        "# plot caa diffs via wandb\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(list(caa_diffs.keys()), list(caa_diffs.values()))\n",
        "ax.set_ylabel(\"prob diff\")\n",
        "ax.set_title(\"caa: change in token probs\")\n",
        "plt.tight_layout()\n",
        "wandb.log({\"caa_plot\": wandb.Image(fig)})\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xPFu5xDiqIsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucmhCQn7phAl"
      },
      "outputs": [],
      "source": [
        "# 50-token rollout (mdp sim)\n",
        "def generate_rollout(prompt, steering_vector=None, alpha=1.0, length=50):\n",
        "    gen_toks = []\n",
        "    cur_prompt = prompt\n",
        "    for i in range(length):\n",
        "        inputs = tokenizer(cur_prompt, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs, output_hidden_states=True)\n",
        "        hidden = outputs.hidden_states[-1].mean(dim=1).squeeze().detach()\n",
        "        if steering_vector is not None:\n",
        "            hidden = hidden + alpha * torch.tensor(steering_vector, dtype=hidden.dtype)\n",
        "        logits = hidden @ model.transformer.wte.weight.T\n",
        "        next_id = torch.argmax(logits).item()\n",
        "        next_tok = tokenizer.decode([next_id])\n",
        "        gen_toks.append(next_tok)\n",
        "        cur_prompt += next_tok\n",
        "    return \"\".join(gen_toks)\n",
        "\n",
        "baseline_rollout = generate_rollout(\"the movie was\", steering_vector=None, length=50)\n",
        "intervened_rollout = generate_rollout(\"the movie was\", steering_vector=C, alpha=1.0, length=50)\n",
        "print(\"baseline rollout:\\n\", baseline_rollout)\n",
        "print(\"\\nintervened rollout (w/ c):\\n\", intervened_rollout)\n",
        "\n",
        "wandb.log({\n",
        "    \"baseline_rollout\": baseline_rollout,\n",
        "    \"intervened_rollout\": intervened_rollout\n",
        "})\n",
        "\n",
        "wandb.finish()\n",
        "print(\"exp logged w/ wandb\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xJOkVfFlqLU4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}