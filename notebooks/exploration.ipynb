{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def load_gemma(model_name=\"google/gemma-2-9b\", device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Load Gemma model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model identifier\n",
    "        device: Device to load the model on\n",
    "    \n",
    "    Returns:\n",
    "        model: Loaded model with hooks for activation access\n",
    "        tokenizer: Corresponding tokenizer\n",
    "    \"\"\"\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,  # Use fp16 for efficiency\n",
    "        device_map=device\n",
    "    )\n",
    "    \n",
    "    # Set evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
